# ID2223 - [Lab 1](https://github.com/ID2223KTH/id2223kth.github.io/tree/master/src/serverless-ml-intro). 

In this Lab, we created and deployed two serverless end-to-end machine learning pipelines: from loading the datasets to batch predictions (for model monitoring) as well as immediate predictions given the respective feature values inputted in a GUI. The first pipeline was provided and uses [this](https://repo.hops.works/master/hopsworks-tutorials/data/iris.csv) Iris dataset, and the second pipeline borrows code from the Iris pipeline, but uses a feature store and model based on [this](https://raw.githubusercontent.com/ID2223KTH/id2223kth.github.io/master/assignments/lab1/titanic.csv) Titanc dataset.

## Pipeline architectures

The two pipelines use the same architecture as depicted below. Essentially, the only difference is the data and therefore also how the data is preprocessed, what ML algorithm is used and ultimately the components of the GUI. Following the picture, short summaries about each pipeline component are characterized.

<p align="center">
<img width="600" alt="Skärmavbild 2022-11-24 kl  16 46 17" src="https://user-images.githubusercontent.com/50402197/203823586-dd4134b9-fd62-4b33-8525-d3697a68495d.png">
<img width="600" alt="Skärmavbild 2022-11-24 kl  16 47 21" src="https://user-images.githubusercontent.com/50402197/203823814-e876bb89-35d7-43bf-a9f5-0b9d118618fc.png">
</p>

### Feature pipeline

In this pipeline, reading data from a source (e.g. from an API call or locally) and creating a dataframe out of it is done. Afterwards, an exploratory analysis can provide an idea of the data types, missing values and data distribution. In our case, for the Titanic dataset, the exploratory analysis was done in a locally hosted jupyter notebook. Given the information generated by the exploratory analysis, data preprocessing is done and subsequently the features that the to-be model will use during training. Out of this, we create a feature group in Hopswork and insert the preprocessed data. 

### Training pipeline
After running this pipeline, the features from the feature store are read. We fetch a feature view to retrieve the target/class values if it exists, otherwise we create a new feature view and get the class values. Subsequently, the data is split, a model is trained and its performance is evaluated. The model and its performance is then stored for further usage of other modules. 

### Feature pipeline daily
This pipeline is for checking the health of our model by feeding unobserved instances to the model. We do this by creating an artificial instance and append it to the feature group in Hopsworks. We create a synthetic passenger and the values are generated at random and added to the feature group in Hopsworks.

### Batch inference pipeline
The task for this module is to run the model on a user defined basis (as a job). This pipeline is for making predictions on the batch data collected from the feature group from hopsworks and also show histroical performance of our model along with confusion matrix. This pipeline runs on a batch data which also includes the new synthetic passenger. The output shows the predicted and the actual label of the synthetic passenger data. The values get updated with each execution. This pipeline is scheduled as a job on Modal running once per day.

## Setup
1. Create an account on [Hopswork](https://www.hopsworks.ai/) and generate an API key.
2. Create an account on [Modal](https://modal.com/) and add HOPSWORKS_API_KEY as a Environment
variable secret.
3. Create an account on [Huggingface](https://huggingface.co/), create a space and create a Gardio app. Add your HOPSWORKS_API_KEY as a Repo Secret. 
4. Fork this repo.
5. Create a virtual environment and install dependencies `pip install requirements.txt`.
6. Run the files (e.g. `pyton feature-pipeline.py) one by one.

## Links

### Iris 

_User interface links (Hugging Face)_
- Dashboard
    - https://huggingface.co/spaces/akseluhr/iris-monitoring
    - https://huggingface.co/spaces/Abdullah1428/iris-monitoring
- Interactive
    - https://huggingface.co/spaces/akseluhr/iris
    - https://huggingface.co/spaces/Abdullah1428/Iris
- Code
    - Source code in `./iris`

### Titanic

_User interface links (Hugging Face)_
- Dashboard
    - https://huggingface.co/spaces/akseluhr/titanic-monitoring
    - https://huggingface.co/spaces/Abdullah1428/titanic-monitoring
- Interactive
    - https://huggingface.co/spaces/akseluhr/titanic
    - https://huggingface.co/spaces/Abdullah1428/titanic
- Code
    - Source code in `./titanic`

## Titanic feature and model training 
This final section describes how we find a model for the Titanic dataset.

### Feature extraction
An initial exploratory analysis indicated relevant features to the target variable and provided insights about the data types and missing values of the dataset. Seen in the picture below, the heatmap gives an indication that the features "Fare" and "PClass" are the most important features for describing "Survived". However, we chose to keep the "Age" feature as it increased the accuracy with .02 despite any correlation with "Survived". The other features were dropped. 

<p align="center">
<img width="412" alt="Skärmavbild 2022-11-24 kl  17 22 49" src="https://user-images.githubusercontent.com/50402197/203830176-1eef925c-bff6-44d1-b868-3a4fd5e6109e.png">
</p>

### Data preperation
We filled the missing values for age with the mean for this column. Furthermore, we binned the values (really not sure about the motivation behind binning). After a 85/15 split of the data, we normalized all values to the same scale between 0-1 as it is recommended to enhance the model's accuracy.

### Hyper parameter tuning & model training
Before training the model (we used a RandomForestClassifier form scikit-learn), we did a coarse/random grid search and a refined grid search for finding the parameters. This grid search uses a cross validation technique (5 folds) to evaluate the performance for each model. The best parameters were ultimately selected for training the final model. 
